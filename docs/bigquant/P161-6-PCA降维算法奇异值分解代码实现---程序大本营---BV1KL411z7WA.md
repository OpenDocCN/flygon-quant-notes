# P161：6-PCA降维算法奇异值分解代码实现 - 程序大本营 - BV1KL411z7WA

![](img/24928dee5d8ddcf3a55531d477a06d76_0.png)

来回到咱们课件当中，那我们写完代码了，咱们具体就看一下pca降维的原理是吧。

![](img/24928dee5d8ddcf3a55531d477a06d76_2.png)

那这个斜方差散度矩阵在这儿呢，咱们都要进行介绍。

![](img/24928dee5d8ddcf3a55531d477a06d76_4.png)

样本均值，这个简单是吧，样本方差看到了吗，样本方差你现在能够发现是吧，这有一个n减一。

![](img/24928dee5d8ddcf3a55531d477a06d76_6.png)

那么我们回到咱们的标准差。

![](img/24928dee5d8ddcf3a55531d477a06d76_8.png)

这啊大家看啊，你现在就能够看到这个总体样本，总体的这个标准差，看到了吧，是n分之x2 减x上面带一个杠，是不是代表平均值呀，平方那这个标准差是不是就是方差的开平方呀，你看这还给了一个叫样本的标准差。

那样本和咱们的总体是什么关系呀，你知道样本和咱们的总体是什么关系吗，它是部分和整体的关系，因为我们你想咱们的数据它能代表全部吗，不能是不是啊，我们现在接触的所有数据，它其实都是一个样本。

它不能代表全部数据，对不对，你就好比咱们之前所说的原文，花这个样圆花我们这个数据，你想它是不是全世界所有原文化的一部分呀，那么它其实就是一个样本上面的总体，就包含所有。

那么我们总体的这个标准差它是除以n，而咱们样本的标准差是除以n减一，为什么要这样做呢，唉这个就是为了避免产生误差啊，为了避免产生误差，所以说我们在进行操作的时候，这个分母上咱们把它变大一些，看到了吗。

分母上把它这个分母上变小，整体是不是就变大了，这是一个近似，估计你有发现你看这个地方是不是一呀，还记得我刚才问你的那个问题吗。



![](img/24928dee5d8ddcf3a55531d477a06d76_10.png)

在代码当中我问了你一个这样的问题，我说这个d d o f代表什么，这个d d o f，这个d d o f表示的就是咱们的分母上，它减去的这个数值，很显然pca这个算法，咱们再进行vn操作的时候。

这个pc这个算法，咱们在进行这个归一化处理的时候。

![](img/24928dee5d8ddcf3a55531d477a06d76_12.png)

我们分母上所除的就是n减一。

![](img/24928dee5d8ddcf3a55531d477a06d76_14.png)

而默认情况下咱们计算的这个标准差，他的d d o f默认是零，你如果默认是零，你看咱们计算出来的结果，和下面这个就稍微有点偏差，但是很接近，是不是你看1。3097，这个是1。3053。

那我们让它的分母减去除以一个n减一，让这个等于一，那这个时候计算出来的结果就一亩一样，都一模一样了，那pca里边不就是这么个封装吗，对不对。



![](img/24928dee5d8ddcf3a55531d477a06d76_16.png)

是不是啊，唉所以说这里边的概念就涉及到了总体样本嗯，总体标准差和样本标准差，这是数学上是吧，数学家比较较近的地方是吧，因为样本是吧，你样本你不可能代替全部，那不能代替全部，一定会有一定的误差。

为了防止这个误差怎么样，我们n减一分母上是吧，变小一些，整体就稍微偏大一些，那整体稍微偏大一些，咱们呢这个得到的结果就更可能接近，咱们总体的标准差，唉所以说呢才会有这样的一个操作。



![](img/24928dee5d8ddcf3a55531d477a06d76_18.png)

现在你明白为什么这个给一个d d o f了吧。

![](img/24928dee5d8ddcf3a55531d477a06d76_20.png)

它就代表分母上的这个n减一，如果你要是零，那是不是就是n减零呀，对不对。

![](img/24928dee5d8ddcf3a55531d477a06d76_22.png)

其实呢在咱们进行这个操作和处理的时候，你把这个变成零也没关系，差别不是特别大，知道吧，当然了，我们知道了这个参数是什么含义，那你再进行操作和处理的时候，你加这个参数和不加这个参数，你去看一下它的差别。

你就能知道是吧，这个微小的一个改变，有可能对于你分值的提升是吧。

![](img/24928dee5d8ddcf3a55531d477a06d76_24.png)

是有一定帮助的，好来回到咱们的课题当中啊。

![](img/24928dee5d8ddcf3a55531d477a06d76_26.png)

好那么这个样本x和样本y的协方差呢。

![](img/24928dee5d8ddcf3a55531d477a06d76_28.png)

就是咱们这个公式好，那么。

![](img/24928dee5d8ddcf3a55531d477a06d76_30.png)

我们除了这个两个属性之间计算呢。

![](img/24928dee5d8ddcf3a55531d477a06d76_32.png)

咱们还有这个嗯还有什么呢。

![](img/24928dee5d8ddcf3a55531d477a06d76_34.png)

还有这个散度矩阵，看咱们cn y是吧，x y z呢，咱们其实就是你看咱们在进行这个计算的时候。

![](img/24928dee5d8ddcf3a55531d477a06d76_36.png)

刚一开始我们就给各位介绍了相关性，是不是你看咱们的相关性计算，看看咱们的相关性计算，你就能够发现看到了吧，斜对角线上，看看斜对角线上是不是自己和自己进行比较呀，所以自己和自己进行比较是不是都是一呀。

而如果我们的这个嗯协方差。

![](img/24928dee5d8ddcf3a55531d477a06d76_38.png)

看咱们斜方差如果要自己跟自己算的话，你看到了吧，cn cn v x和x是吧，c v y和y c o v z和z是吧，那其实你这个不就相当于是方差吗，是不是斜方差，方差呢是斜方差的一种特殊形式。



![](img/24928dee5d8ddcf3a55531d477a06d76_40.png)

好那么介绍了样本方差，介绍了样本均值。

![](img/24928dee5d8ddcf3a55531d477a06d76_42.png)

介绍了斜方差，接下来呢我们继续往下看啊。

![](img/24928dee5d8ddcf3a55531d477a06d76_44.png)

咱们呢再看一下数据x的这个散度矩阵。

![](img/24928dee5d8ddcf3a55531d477a06d76_46.png)

那么什么是散度矩阵呢，嗯其实协方差矩阵和散度矩阵的关系密切，散度矩阵呢就是协方差矩阵乘以总数据量，n减一，那因此它们的特征值和特征向量是一样的，那么这里值得注意啊，散度矩阵是其一直分解的一步。

因此pca和svd是有很大关系的，我们刚才介绍的pca，咱们是不是特征值和特征向量的一个分解呀，对不对，看咱们刚才介绍的p ca呢。



![](img/24928dee5d8ddcf3a55531d477a06d76_48.png)

是特征值和特征向量的一个分解好。

![](img/24928dee5d8ddcf3a55531d477a06d76_50.png)

那么在这里呢，这个pca的算法还有另外的一种方式。

![](img/24928dee5d8ddcf3a55531d477a06d76_52.png)

这个p c a的算法呢，看还有哪还有哪一种方式呢，这个还有使用咱们的s v d奇异值分解，来实现pca降维的算法。



![](img/24928dee5d8ddcf3a55531d477a06d76_54.png)

有两种方式好，那么现在呢咱们看一下第二种方式。

![](img/24928dee5d8ddcf3a55531d477a06d76_56.png)

ctrl a ctrl c看啊，现在我就将这个代码复制一下。

![](img/24928dee5d8ddcf3a55531d477a06d76_58.png)

回到咱们代码当中，刚才呢这个其实叫pca的原理是吧，这个叫pca的代码实现，我们用的方式呢就是特征值，特征向量，好那么接下来我们再给一种方式，那就是pca它的代码实现，这回呢咱们用的是奇异值分解。

那奇异值分解你还有概念吗，我先问一下啊，奇异值分解大家还有概念吗，还有印象吗，是不是听到这个奇异值分解，你是不是有点印象呀，这个也是线性代数当中的一个知识点，现在呢我们从cp当中导了一个包。

大家注意这个如果你没有的话，那你就pip引到cp，你能够看到这个cp呢和什么特别像呢，是不是和nip特别像呀，那这个南派呢它是基本的嗯，它是基本的这个数据计算库，上面这个cp呢它呢是高级的数据计算库。

这里边有非常多的算法，这个s cn呢就是science科学的意思，所以我们从cp下边导入零line，它里边就有相应的这个方法好，那么第一步呢依然也是去中心化，和刚才一样。

然后接下来呢咱们进行了奇异值分解，看那就是奇异值就是零line s v d，这就是极值分解，得到了u s和vt，接下来呢咱们进行了一个符号的翻转，如果为负，那么咱们就把它变成正值。

看就是max abs np。x这个np。arg max，我们让这个np点呃，这个你看这个np。sun呢，它其实就是一个符号函数，看这个就是一个符号函数，然后呢我们降为特征筛选。

你看这个和咱们刚才p ca是不是一模一样啊，然后我们又进行了个什么啊，然后咱们就进行了这个规划操作，执行一下这个代码诶，你现在发现看，现在发现这个结果和咱们刚才的一样不一样。

看咱们将x下划线pca中括号冒号来一个五，你看我一直行，诶，大家发现是不是完全一样呀，看到了吧，1。3053，1。3053，你看第二个是0。6483，这个是多少，0。6483a正负号是不是也完全一样了。

刚才咱们使用第一种方式的时候，咱们说你看中间这个0。6，我们和它的正负是不是刚好相反，那么我们往上走啊，慢慢我都会给你解释，shift tab，咱们点开这个加号，咱们看一下咱们的pca看啊。

你看pca它的svd server默认选的是什么auto，我们看一下这个auto表示什么啊，我们往下滑看看看咱们这个这个s v d server auto，full a pack。

我们看一下这个auto表示什么啊，if auto the server is selected selected by a default，policy based extra ship。

嗯这个呢是根据咱们数据的大小来进行的，是不是看if复看，if负run以act for svd，你看到了吧，如果要是负的话，我们会怎么样，是不是就会使用这个svd，这种方式来进行操作呀，看到了吧。

你看我们现在默认给的是auto，咱们看一下exership at n components，if负责input data is a layer than，你看如果他要是500x500的话。

the number of coonents to extract is lower than，那就会是80%是吧，那otherwise，如果我们的数据是吧，没有超过500x500，咱们会怎么样呢。

是不是就会使用这个负svd呀，那你想咱们的数据x它超过500x500了吗，看一下咱们x。sheep，你看它超过了吗，是不是就是150和四呀，所以这个时候咱们的svd server，虽然你给的是auto。

其实我用的是什么，其实我用的是不是full呀，看到了吧，如果我要用的是full会怎么样呀，你看如果你要是full，你看会怎么样，你如果要是负的话，咱们是不是就用的svd啊，s v d表示什么。



![](img/24928dee5d8ddcf3a55531d477a06d76_60.png)

svd表示什么，svd就表示咱们的奇异值分解，那什么是奇异值分解呀。

![](img/24928dee5d8ddcf3a55531d477a06d76_62.png)

看咱往上滑动啊，看一下什么是奇异值分解，svd呢是一种因子分解运算，还记得我刚才给你举的例子，六等于什么，2x3，是不是矩阵也可以分解，咱们将一个矩阵分解为三个矩阵的乘积，看三个矩阵就是u西格玛v。

注意啊，这个符号念西格玛是吧，其中u和v是正交矩阵，分别称为左奇翼右旗一西格玛为奇异值，看到了吗，西格玛为奇异值，那奇异值分解，咱们的a呢也就约等于u乘以西格玛乘以v的，v的转置乘以v的转置。



![](img/24928dee5d8ddcf3a55531d477a06d76_64.png)

所以说呢唉这个就是奇异值分解。

![](img/24928dee5d8ddcf3a55531d477a06d76_66.png)

我们刚才在代码当中，是不是进行了奇异值分解呀，往下滑看到了吧，这个是不是就是咱们奇异值分解，对不对，其实分解咱们得到的结果是不是u s和vt啊，那么x可以用什么来表示呢，我们在下面插入一行。

看x可以用什么来表示啊，咱们下面插入一行，咱们让u。dot一下咱们的s卡，你看这不是s吗是吧，然后呢再来dot一下什么，咱们呢再来dot一下vt，这个时候我查看它的前五个，你看我一执行嗯啊我看一下啊。

咱们进行了看咱们的u，我们是进行了相应的这个运算，对不对呀，所以说呢咱们把这行代码复制一下，我们给它放到这儿重新计算，新的嗯，我看一下啊，150 四not ali。



![](img/24928dee5d8ddcf3a55531d477a06d76_68.png)

看执行一下，这个时候咱们就得到一个结果。

![](img/24928dee5d8ddcf3a55531d477a06d76_70.png)

是不是啊，然后呢咱们再来dot一下啊，dot小括号微v t。t执行一下呃。

![](img/24928dee5d8ddcf3a55531d477a06d76_72.png)

154，not alone，咱们看一下我们的v t长什么样啊，现在呢我们先将其值分解的这几个数据的形状，咱们给它显示一下s。ship，然后vt。ship执行一下这个代码，哎现在我们就能够发现。

你看咱们是不是140嗯，中间这个是四，对不对呀，然后呢，你看我们这个看这个是不是也是四和四呀，那你知道这个s代表什么吗。



![](img/24928dee5d8ddcf3a55531d477a06d76_74.png)

这个s根据我们根据咱们这个公式看到了吧，这个s是不是就代表咱们的西格玛呀，那这个sigma你看他是不是应该得是矩阵才行呀。



![](img/24928dee5d8ddcf3a55531d477a06d76_76.png)

对不对，你看它得是矩阵才行，那咱们怎么把它转换一下呀，那打印输出一下啊，看咱们print一下咱们的s让你看一下，你看他是25是吧，6。03。4，也就是说这个s是奇异值。



![](img/24928dee5d8ddcf3a55531d477a06d76_78.png)

那你如果要是奇异值的话，你看看啊，大家看一下咱们这个奇异值，它的形状看到了吧，这是什么，n乘以n，对不对，看到了吧，n乘以n，n乘以n是不是就表示了它的形状呀，但是咱们现在。

我们算法为我们返回的是一个数据对吧，那怎么办，来咱们现在把它转成矩阵，转成矩阵之后，咱们让它斜对角线上好，我们让它的斜对角线上是这些，我们让它斜对角线上是特征值。



![](img/24928dee5d8ddcf3a55531d477a06d76_80.png)

其余的地方我们让它全部是零，那么这个时候呢咱们回到代码当中，np里面呢有一个方法，np里面有一个方法叫做e y e，咱们呢来一个小括号啊，shift tab啊，咱们就给一个数据。

那这个数据呢咱们就给一个四这个，然后呢我们给一个s嗯，我们看一下啊，呃这个时候呢不太行是吧，np点，np点我们来一个zero，咱们让他的sheep呢就等于四，那让他的ship等于4x4啊。

小括号来一个四逗号四哎，这个时候我们就发现，你看咱们的数据是不是就有了，那么有了这个数据之后呢，咱把s呢给它放到它的斜对角线上，那怎么才能放到它的斜对角线上呢，咱们来一个for循环，先接收这个数据是吧。

这个数据呢咱们就叫西格玛，现在这个西格玛呢它全是零，对不对，看sigma全是零，然后呢我们来一个for循环，咱们呢呃来一个for i，啊for i in range 4。

然后再来一层for循环for j in range 4，那这个时候呢if i看等等咱们的j，那这个时候咱们就赋值呗，那就是像西格玛当中怎么样呃，我们就像西格玛当中是吧，a逗号j是吧，我们让它等于多少。

等于咱们的s中括号，咱们把i放进去，这个时候你再来看咱们的西格玛啊，你看我一直行sigma的斜对角线上怎么样，是不是就放成这个数据了，放成这个数据之后，咱们接下来就开始这个矩阵乘法，那就是优点dot哎。

大家拭目以待啊，优点dot，我们将西格玛放进去，然后呢再来一个点dot，咱们将vt放进去，此时你看过一执行嗯是吧，然后呢咱们冒号来前五个数据啊，啊我们得到的这个数据呢，咱们来一个点t啊。

这个时候执行一下嗯，大家现在就能够看到呃，咱们得到的负的0。91，负的0。912点，看u西格玛vt我们计算出来的这个结果，看执行一下啊，看这个时候咱们计算0。23，0。24和咱们的数据x很显然是对不上。

是不是我们在上面插入一行啊，x中括号冒号，我们看咱们原始数据的，你看看一下咱们原始数据啊，看原始数据x是不是负的0。74，负的0。94，看到了吧，负的0。94，负的1。14，你现在明白看啊。

现在明白它是怎么一回事儿了吗，我看一下啊，咱们有同学有一个疑问是吧，如果把for循环里的西格玛换成g和i是吧，呃换成j和i也是一样的啊，你知道为什么吗，你看换成j和i也是一样的呃。

因为我们的条件判断是i等等g啊，明白吗，哎判断条件是i等等g这时候各位小伙伴，你来发现看到了吧，数据x和咱们奇异值求解出来的一样不一样。



![](img/24928dee5d8ddcf3a55531d477a06d76_82.png)

看和上面是一模一样的，诶现在你明白这什么叫做奇异值分解了吧。

![](img/24928dee5d8ddcf3a55531d477a06d76_84.png)

你看公式啊，你看公式也就是说我要加一个矩阵，把它分解成三个矩阵相乘得到的结果，看到了吧，把一个矩阵分解成两个矩阵相乘的结果哦。



![](img/24928dee5d8ddcf3a55531d477a06d76_86.png)

你说把它换成爱和爱是吧，那当然也可以啦，你想一下是不是你看把它换成爱和爱，当然也没问题啦，一样的啊，一样的可以，因为i和j相等。



![](img/24928dee5d8ddcf3a55531d477a06d76_88.png)

你看啊，七一直，你现在呢有可能会对于我们的奇异值分解，有一定的遗忘，那你去百度上找奇异值分解的相关资料，也可能看的比较懵逼，现在呢咱们将其一直给你进行了分解，我呢又在代码当中把它的这个方程。

在这里给你进行了一个演示，你看这个地方是约等于啥，是约等于啊，就是你不可能完全一样，知道吗，没有可能完全一样，哪怕你哪怕你是差差一点点是吧。



![](img/24928dee5d8ddcf3a55531d477a06d76_90.png)

唉他也是约等于，你看咱们是不是对于这个数据，x进行了s v d的奇异值分解呀，对不对，我对它进行了奇异值分解，那我是不是还可以进行这个反向乘法呀，看到了吗，咱们再进行一个反向乘法是吧。

反向的矩阵乘法是不是又回到，你看反向乘法是不是又回到x了，咱们得到的这个x你看为什么是负的0。74。

![](img/24928dee5d8ddcf3a55531d477a06d76_92.png)

负的0。94呢，这个x不是咱们的圆微花吗，对吧，x是不是我们的原文花，你记得我们对原文花做了什么样的一个操作吗，我们是不是让他捡，让他去了一个中心化呀，去完中心化之后，咱们的数据就是这个样。

所以奇异值分解，现在你大概明白是怎么一回事儿了吧是吧，就是将一个矩阵，分解成三个矩阵。

![](img/24928dee5d8ddcf3a55531d477a06d76_94.png)

那这三个矩阵分别代表什么呀，左奇异，右骑一西格玛为奇异值是吧，那什么是奇异值呢，奇异值和特征值类似，看特征值特征向量，这个奇异值也是你的奇异值越大，那么你的这个嗯这个重要性呢也就越大好。



![](img/24928dee5d8ddcf3a55531d477a06d76_96.png)

![](img/24928dee5d8ddcf3a55531d477a06d76_97.png)

那么我们再回到代码当中啊，来再给大家看一下啊，这我们使用奇异值分解这种方式，这去中心化这一步不用解释，对不对呀，那我们其一直分解这一块，你看这一块儿是不是也不用解释呀，看到了吧。

其一直分解这一块儿也不用解释，因为我们下面告诉你它到底是怎么回事了，对不对，但是这个地方呢你看啊有一个符号翻转啊，这个地方有一个符号翻转，那咱们这个符号翻转呢，我们是对于u进行的操作，u呢是它的左奇异。

那你看咱们在这儿呢就进行了这个np点，arg max啊，我们进行了一个这个arg max嗯，咱们呢就把它的每一列当中的最大值，给它拿出来看啊，这个换成因式分解可能更好理解一些是吧。

所以我们在最开始的时候就举了一个通俗易懂，简单的例子，那就是6=2x3矩阵分解和这个类似啊，大家知道啊，咱们的矩阵分解和我们这个类似，但是呢很多小伙伴可能已经忘了这个矩阵分解。

可能已经忘了这个s v d代表啥了是吧，在这儿呢咱们又进行了一个演示好，那么我们这个符号翻转，我们之所以进行这个符号翻转，是为了和pca它的结果保持完全一致啊，是为了和pca的结果保持完全一致。

那么特征筛选之后呢，我们就进行了一个规划，那么在规划这有两种计算方式，第一种是大家最常用的就是u减去u mean，除以u。s t d，你看我执行一下啊，好执行一下啊，现在你发现大家看啊，上面这种归一化。

我们求解出来的结果，和下面是不是稍微有一点点的差异呀，怎么办，同样是不是也是d d o t让他等于一，是不是就可以了，看看咱们优点s t d是吧，呃我看一下啊。

咱们这个里边儿啊d d这是d d o f是吧，咱们将d d o f来操作一下，到现在你来看怎么样，是不是完全一样了呀，看到了吧，这第二列看到咱们的第二列了吗，你看咱们的第二列是不是符号也都完全一样了。

那它是怎么操作的，哎就是通过这个符号翻转，咱们呢np。arg max把这一列的最大值找到，然后呢调用np。sun，这个sun是什么函数呢，来我在这给你演示一下啊，看np。sn s i g n。

来一个中括号一二-2是吧，然后呢来一个负的0。5，看这个时候你看我一执行嗯，你看唯一运行哎，我们返回的结果会是什么样呀，一和二是不是都是正的，那么它就是正一，你的-2-0。5，不管你是负的多少。

你是不是都是负的，所以这个叫什么，这个叫符号函数，那么如果我们把咱们这个u当中这个数据呢，我们给他拿到，拿到之后嗯，咱们取出来这个数据是吧，如果说他要是小于-1的，咱们怎么样让u乘以这个三。

其实这个符号不翻转也没事儿，不翻转的话会怎么样呀，看啊ctrl加反斜杠不翻转，你看我执行一下，不翻转和咱们这个结果是不是正好差一个负号，看你这个结果是多少，负的0。6，这个呢是不是正的0。6，仅此而已。

求解出来的数据是不是完全一样，对不对，求解出来的数据完全一样啊，好，那么这节课呢，咱们介绍了pca代码实现的两种方式，这个里边儿肯定还有一些细节，各位需要仔细的去打印输出，知道它到底是怎么回事儿。

今天晚上呢咱们就先到这里啊，今天晚上咱们的代码也不是特别多，但是需要理解的地方特别多，嗯各位小伙伴呢好，那么咱们呢就先到这里啊，好那么呃我们一生一生守候这位小伙伴呢，有一些疑问，咱们直接在我们q呃。

在微信的讨论群里边，你直接发啊，嗯这里边的图片留不住啊，好那么我们今天呢咱们就到这里啊，啊我们的作业呢唉大家注意啊，我们的作业呢，因为我们这个pca呢两者都比较难是吧，涉及到线性代数当中的好多知识点。

大家呢把这两种方式是吧，你搞清楚弄明白啊，符号翻转到底是怎么回事是吧，你把它每一步运行结果都打印输出，自己好好的整理一下好。



![](img/24928dee5d8ddcf3a55531d477a06d76_99.png)