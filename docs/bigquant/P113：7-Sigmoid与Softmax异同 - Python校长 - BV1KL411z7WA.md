# 7天爆肝整理！AI量化交易-机器学习全套教程，从入门到项目实战保姆级教程！（数据挖掘分析／大数据／可视化／投资／金融／股票／算法） - P113：7-Sigmoid与Softmax异同 - Python校长 - BV1KL411z7WA

那麼我們就看最後壹部分啊，邏輯回歸與咱們的softmax回歸，它倆呢是同根同源，當我們的k=2的時候，softmax回歸它就退化為邏輯次第回歸了，下面是我的證明過程，我現在寫的這個是softmax回歸。

看咱們的分母上是不是1的多少次me，而分子上是誰呢，1個2個，這個是不是分別計算概率啊，這個就是咱們的softmax回歸啊，那如果當k=2呢，你看這就是咱們寫了兩個是不是啊，寫了兩個。

它下面呢進行了壹個轉化，因為這些都是向量，向量的話咱們可以怎麼做，看我們可以從兩個向量當中都減θ1，同時減θ1，減完θ1之後呢，這個是不是就變成零向量乘以它了，然後你都減θ1都是不變的，看到了吧。

θ2-θ1，θ2-θ1，這個是不是變成零了，對不對，同時減，那麼我們這個方程依然成立，然後我們展開，你上面是吧，你這個樹幹裏邊就表示這個分子，同時展開，你看這就是1唄，因為你1的零次me不就是1嗎。

然後下面這個就是1的θ2-θ1，t的x是吧，這是tx，那我們在上面的公式當中，咱們θ2-θ1，它是不是壹個新的變量呀，這個新的變量，咱們就用θ來表示，新的變量，咱們就用θ來表示，那我們可以用θ來表示。

大家看啊，把θ帶到這個公式當中，θ帶到這個公式當中，它就演變成了這種形式，演變演變成了這種形式，我們壹化解，得到的這個是不是就是咱們邏輯回歸的公式呀，所以說呢，我們剛才在代碼當中計算的時候。

咱們的邏輯回歸和softmax回歸，計算方式不壹樣，但是他們兩個是吧，當我們softmax回歸，當k=2的時候，softmax回歸就退化為邏輯回歸了，好，它呢是這樣的壹個，它是這樣的壹個關系啊，好。

那麽這個呢就是咱們今天所有的內容，我們壹起來看壹下咱們的作業啊，好，那麽咱們今天呢介紹了多分類，是不是啊，今天介紹了多分類，那這個是我們上壹節課的啊，上壹節課，咱們是不是給各位介紹了葡萄酒分類。

是不是啊，上壹節，那我們這壹節課，咱們的作業呢，大家看啊，這個就類別就有點多了，妳在算的時候呢，妳就得好好算壹下，咱們使用邏輯4D回歸實現手寫數字多分類，這個手寫數字多分類，它就是0123壹直到9。

數據呢已經為各位準備好了，在咱們digital。csv百度網盤當中，我們有這個數據，看啊這是今天的這個數據，看這個數據裏邊數據還不少呢，73。2兆，我們壹共是4萬2000個數據。

妳需要使用咱們所講的代碼，是吧，把這個數據拆分，這第壹列呢是目標值，它是分十類，後面全部是數據和特征，這個數據特征比較多，咱們是784個特征數據，使用train test split。

咱們拆分成訓練和測試數據，然後使用邏輯回歸，建模訓練預測並預測類別，然後我們手動計算概率，這次咱們計算概率的時候，咱們就使用softmax，這個函數這個方式來進行計算，那妳就要對比壹下。

妳手動計算的和上面預測的準確率，它應該完全壹樣才對，知道嗎，我們的類別比較多，所以說這個方程也就比較多，每個方程它都對應著壹個概率，每個類別也都對應著壹個概率，因為我們是多分類，是十個類別。

十個類別對應多少個方程，根據咱們今天所講的，三種原微花對應三個方程，十種數字是不是就對應十個方程，對不對，所以這就是咱們今天要完成的作業，好，那麽今天晚上咱們講課的內容呢，我們就到這裏了。

其實我們課程當中，有壹部分咱們沒有講，我們相應的這個推導公式都在這裏，妳可以自己去研究，可以自己去看壹下。

![](img/a0a22610c882aa69628cb0e4120487ef_1.png)

好，咱們今天晚上就到這裏了。